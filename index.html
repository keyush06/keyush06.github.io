<!DOCTYPE html>
<html>

<head>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="stylesheet" href="style.css">
	<title>Keyush Shah</title>

	
</head>

<body>
	<img id="profile_pic" src="assets/Pinpoint_DSC_0590.jpg" alt="Keyush">

	<h1>Keyush Shah</h1>

	<h3>
		<a href="assets/Resume__Keyush_HTMRe.pdf" class="red-link">Resume</a>
		| <a href="https://scholar.google.com/citations?user=MLffnpkAAAAJ&hl=en" class="red-link">Google Scholar</a>
		| <a href="https://github.com/keyush06" class="red-link">Github</a>
		| <a href="mailto:keyush.shah12@gmail.com" class="red-link">Email ID</a>
	</h3>
	

	<br>

	<p>
	Hello, I am a second year graduate student at the University of Pennsylvania pursuing MSE in Data Science. I am a Research Assistant at the <a href="https://csl-lab-upenn.github.io/">Computational Social Listening Lab</a> where my research is focussed on identifying and detecting misinformation across social media platforms and how it distributes across social communities and networks.
	I am advised by Prof. <a href="https://www.cis.upenn.edu/~ungar/">Lyle Ungar</a> and Prof. <a href="https://sharathg.cis.upenn.edu/">Sharath Guntuku</a>.
	</p>

	<section>
		<h3 style="text-decoration: underline; color: gray;">Research Interests</h3>
	
		<p>
			<strong>1. Vision-Language Understanding:</strong> I am interested in pursuing research on enhancing 
			<strong>Vision-Language Models (VLMs)</strong> through Representation Learning and Transfer Learning, 
			with a focus on <strong>Compositionality</strong>, <strong>Alignment</strong>, and <strong>Grounding</strong> 
			to better integrate visual and textual data.
		</p>
	
		<p>
			<strong>2. Responsible VLMs:</strong> I aim to explore ways to address biases and hallucinations in 
			VLMs by establishing benchmarks for fairer and more ethical systems, contributing to the 
			development of inclusive and trustworthy Social AI.
		</p>
	
		<p>
			<strong>3. Datasets and Synthetic Data:</strong> I am keen on investigating the role of datasets in building 
			VLMs, particularly the potential of synthetic data generation to overcome limitations and 
			enable diverse, scalable, and high-quality training resources.
	</section>
	

	<div style="border: 2px solid #808080; padding: 15px; margin: 20px 0; border-radius: 8px;">
		<h3 style="background-color: #FFD700; color: black; margin: -15px -15px 10px -15px; padding: 10px; border-radius: 8px 8px 0 0; text-align: center; font-size: 1.2em;">
			Current work in Progress
		</h3>
        <p>
			
			In this regard, I am exploring Multimodal Learning Models (MLMs) that are at the forefront of AI research. 
            Inspired by papers like <a href="https://github.com/chenhaoxing/DeMamba">DeMamba</a> and 
            <a href="https://arxiv.org/abs/2304.08485">Llava</a>, I am focusing on a task of generating videos with reasoning capabilities. 
            The broader research question is how Representation Learning takes place in the case of multiple modalities.
		
        </p>
    </div>


	<div style="border: 2px solid #808080; padding: 15px; margin: 20px 0; border-radius: 8px;">
		<h3 style="background-color: #003366; color: white; margin: -15px -15px 10px -15px; padding: 10px; border-radius: 8px 8px 0 0; text-align: center; font-size: 1.2em;">
			Deviating from AI
		</h3>
		<p>
			On the business end, I am familiar with leveraging statistical models to drive impactful business decisions, 
			such as predicting customer lifetime value (CLV), performing customer segmentation, and developing forecasting models. 
			Courses like Applied Statistical Models in Modelling at Wharton equipped me with advanced techniques for building 
			behaviorally plausible models to explain and predict consumer patterns.
		</p>
	</div>
	<!--<p>-->
	<!--I am interested in deep learning research, with a recent focus on foundation models. In my past work, I have enjoyed forays into noisy data learning, graph ML, neuro-symbolic AI, and program analysis.-->
	<!--</p>-->

	<!-- <h2>news</h2>

	<table>

	<tr>
		<th>Jul 30, 2024</th>
		<td><a href="https://relbench.stanford.edu">RelBench</a> (v1) released.</td>
	</tr>

	<tr>
		<th>May 1, 2024</th>
		<td>Our <a href="assets/2024_icml_rdl/paper.pdf">Relational Deep Learning</a> position paper was accepted at ICML 2024.</td>
	</tr>

	<tr>
		<th>Apr 11, 2024</th>
		<td>My CMU work on <a href="https://arxiv.org/abs/2404.07815">Post-Hoc Reversal</a> is on arXiv.</td>
	</tr>

	<tr>
		<th>Mar 18, 2024</th>
		<td>I aligned with Prof. <a href=https://cs.stanford.edu/people/jure/">Jure Leskovec</a> as my PhD advisor.</td>
	</tr>

	<tr>
		<th>Nov 28, 2023</th>
		<td>Our <a href="https://arxiv.org/abs/2312.04615">Relational Deep Learning</a> paper was on the front page of <a href="https://news.ycombinator.com/item?id=38447338">Hacker News</a>.</td>
	</tr>

	<tr>
		<th>Nov 27, 2023</th>
		<td><a href="https://relbench.stanford.edu">RelBench</a> (Beta) released at Learning on Graphs (LoG) conference.</td>
	</tr>

	<tr>
		<th>Sep 26, 2023</th>
		<td>I started my PhD at Stanford.</td>
	</tr>

	<tr>
		<th>Jul 15, 2023</th>
		<td>This webpage is live.</td>
	</tr>

	</table> -->

	<h2>Professional Experience</h2>

	<ol class="horizontal-list">
		
		<li>
			<h3><a href="assets/Resume__Keyush_HTMRe.pdf"">CSL Lab</a></h3>
			<i>Research Assistant</i>
			<i>2024</i>
		</li>

		<li>

			<h3><a href="assets/Resume__Keyush_HTMRe.pdf"">Universal Media</a></h3>
			<i>Data Science Intern</i>
			<i>2024</i>
		</li>
		
		<li>
			<h3><a href="assets/Resume__Keyush_HTMRe.pdf"">IIFL Finance Ltd</a></h3>
			<i>Assistant Manager</i>
			<i>2022 - 2023</i>
		</li>
	</ol>
	
	


	<h2>Publication: pre-prints</h2>

	(* denotes equal contribution)

	<ol>
	
	<li>
		<b>Enhancing Retrieval in QA Systems with Derived Feature Association</b><br>
		<u>Keyush Shah</u>, <u>Abhishek Goyal*</u>, <u>Isaac Wasserman*</u><br>
		<i>arXiv preprint, 2024</i><br>

		<a href="assets/neurips_2023.pdf">paper</a>
		| <a href="https://arxiv.org/abs/2410.03754">arxiv</a>
		| <a href="https://github.com/isaacwasserman/LongRAG">code</a>
		<!-- | <a href="https://relbench.stanford.edu">website</a> -->
		<!-- | <a href="assets/2024_arxiv_relbench/cite.txt">cite</a> -->
	</li>

	<!-- <li>
		<b>Post-Hoc Reversal: Are We Selecting Models Prematurely?</b><br>
		<u>Rishabh Ranjan</u>, Saurabh Garg, Mrigank Raman, Carlos Guestrin, Zachary Lipton<br>
		<i>arXiv preprint, 2024</i><br>

		<a href="assets/2024_arxiv_phr/paper.pdf">paper</a>
		| <a href="https://arxiv.org/abs/2404.07815">arxiv</a>
		| <a href="https://github.com/rishabh-ranjan/post-hoc-reversal">code</a>
		| <a href="assets/2024_arxiv_phr/cite.txt">cite</a>
	</li> -->

	</ol>


	<h2>Selected Projects</h2>

	<!-- (* denotes equal contribution) -->

	<ol>

	<li>
		<h3>Instance Segmentation: By location</h3>
		<img src="assets\ins_segmentation.jpeg" alt="Intsance Segmentation Pic" style="width:300px;"><br>
		I implemented an advanced instance segmentation framework inspired by the SOLO (Segmenting Objects by Locations) model. It features a ResNet backbone for robust feature extraction and a Feature Pyramid Network (FPN) to handle multi-scale object representations efficiently.

		The architecture consists of two main branches. The Category Prediction Branch assigns pixels to grid-based instance categories, leveraging spatial information to effectively localize and distinguish objects of varying sizes. Meanwhile, the Mask Segmentation Branch generates accurate binary masks using a spatially sensitive, fully convolutional network, eliminating the need for traditional bounding boxes or complex post-processing.

		This end-to-end trainable system simplifies the segmentation pipeline, learning directly from mask annotations to enhance efficiency and deliver high performance across diverse object segmentation tasks. <br>
		<!-- <i>International Conference on Machine Learning (ICML), 2024</i><br> -->

		<!-- <a href="assets/2024_icml_rdl/paper.pdf">paper</a>
		| <a href="https://arxiv.org/abs/2312.04615">arxiv</a> -->
		| <a href="https://github.com/keyush06/Segmenting-Objects-by-Location">code</a>
		| <a href="https://arxiv.org/abs/2106.15947">Reference Paper</a>

		<!-- | <a href="https://relbench.stanford.edu">website</a>
		| <a href="assets/2023_icml_rdl/cite.txt">cite</a> -->
	</li>

	<li>

		<h3>FitBit</h3>
		<img src="assets\health Chatbot.jpeg" alt="healthcare bot" style="width:300px;"><br>
		Designed and developed a Django-based AI chatbot for health-related conversations, integrating PostgreSQL for robust patient data management and Langchain for LLM-agnostic model orchestration. Implemented dynamic entity extraction to capture key details like medications and appointment preferences, optimized memory usage for long conversations, and enabled automated escalation of appointment and treatment requests.  <br>
		<!-- <i>International Conference on Machine Learning (ICML), 2024</i><br> -->

		<!-- <a href="assets/2024_icml_rdl/paper.pdf">paper</a>
		| <a href="https://arxiv.org/abs/2312.04615">arxiv</a> -->
		| <a href="https://github.com/keyush06/FitBit_ChatBot">Github code</a>
		<!-- | <a href="https://arxiv.org/abs/2106.15947">Reference Paper</a> -->


	</li>





	<!-- add this only when needed -->
	<!-- <li>
		<h3>Marvel Bot</h3>
		<img src="assets/marvel.jpeg" alt="Marvel Image" style="width:300px;"><br>
		Engineered a conversational LLM app using LangChain and RAG, integrating Ollama to optimize three quantized models. Web scraped 4+ Marvel Wikipedia pages and deployed the solution on Streamlit, ensuring real-time, context-aware responses and efficient memory usage. <br>
		 <i>International Conference on Machine Learning (ICML), 2024</i><br> -->

		<!-- <a href="assets/2024_icml_rdl/paper.pdf">paper</a>
		| <a href="https://arxiv.org/abs/2312.04615">arxiv</a> -->
		<!-- | <a href="https://github.com/keyush06/Marvel-bot-Conversational-RAG-based-LLM-application">code</a> -->
		<!-- | <a href="https://relbench.stanford.edu">website</a>
		| <a href="assets/2023_icml_rdl/cite.txt">cite</a> -->
	<!-- </li> -->

	<li>
		<h3>Traversability Estimation</h3><br>
		<img src="assets/OffRoadNavigation.jpeg" alt="Terrain Image" style="width:300px;"><br>
		Developed a terrain classification model using Semantic Segmentation and an attention enhanced Fully Convolutional Network, achieving a 2% improvement in IoU. Enhanced off-road navigation for autonomous 
		vehicles by optimizing path planning and terrain adaptability.<br>
		<!-- <i>Advances in Neural Information Processing Systems (NeurIPS), 2022</i><br> -->

		<a href="assets/Traversability Estimation/Report.pdf">paper</a>
		| <a href="assets/Traversability Estimation/Pyramid_pool_Final.ipynb">code</a>
		<!-- | <a href="https://openreview.net/forum?id=EqZuN4V_FLF">reviews</a>
		| <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53251.png?t=1669859105.1475148">poster</a>
		| <a href="https://neurips.cc/virtual/2022/poster/53251">slides</a>
		| <a href="https://github.com/dair-iitd/ilploss">code</a>
		| <a href="assets/2022_neurips_ilploss/cite.txt">cite</a> -->
	</li>

	<!-- <li>
		<b>GREED: A Neural Framework for Learning Graph Distance Functions</b><br>
		<u>Rishabh Ranjan</u>, Siddharth Grover, Sourav Medya, Venkatesan Chakaravarthy, Yogish Sabharwal, Sayan Ranu<br>
		<i>Advances in Neural Information Processing Systems (NeurIPS), 2022</i><br>

		<a href="assets/2022_neurips_greed/paper.pdf">paper</a>
		| <a href="https://arxiv.org/abs/2112.13143">arxiv</a>
		| <a href="https://openreview.net/forum?id=3LBxVcnsEkV">reviews</a>
		| <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54507.png?t=1669859442.0786178">poster</a>
		| <a href="https://neurips.cc/virtual/2022/poster/54507">slides</a>
		| <a href="https://github.com/idea-iitd/greed">code</a>
		| <a href="assets/2022_neurips_greed/cite.txt">cite</a>
	</li>

	<li>
		<b>Exploiting Epochs and Symmetries in Analysing MPI Programs</b><br>
		<u>Rishabh Ranjan</u>, Ishita Agrawal, Subodh Sharma<br>
		<i>IEEE/ACM International Conference on Automated Software Engineering (ASE), 2022</i><br>


		<a href="assets/2022_ase_simian/paper.pdf">paper</a>
		| <a href="https://github.com/rishabh-ranjan/simian">code</a>
		| <a href="https://sat-smt-ws.gitlab.io/2020/programme.html">talk</a>
		| <a href="assets/2022_ase_simian/cite.txt">cite</a>
	</li> -->

	</ol>


	<footer>
		Made with &#10084;&#65039; in plain HTML and CSS code

		<!-- Made with &#10084;&#65039; in plain HTML and CSS (<a href="https://github.com/rishabh-ranjan/rishabh-ranjan.github.io">code</a>). -->
	<footer>
</body>

</html>
